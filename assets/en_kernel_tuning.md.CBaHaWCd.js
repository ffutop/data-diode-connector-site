import{_ as s,c as i,o as a,a6 as n}from"./chunks/framework.Cpr3xyXy.js";const k=JSON.parse('{"title":"Kernel Tuning for 10Gbps Data Diode Performance","description":"High-performance tuning guide for Data Diode Connector (DDC) on Linux. Covers UDP Socket buffers (`rmem_max`), CPU affinity pinning, NIC offloading (GRO/GSO), and interrupt coalescing to achieve 10Gbps line rate.","frontmatter":{"title":"Kernel Tuning for 10Gbps Data Diode Performance","description":"High-performance tuning guide for Data Diode Connector (DDC) on Linux. Covers UDP Socket buffers (`rmem_max`), CPU affinity pinning, NIC offloading (GRO/GSO), and interrupt coalescing to achieve 10Gbps line rate.","head":[["meta",{"name":"keywords","content":"Linux Kernel Tuning, UDP Optimization, CPU Affinity, NIC Offloading, GRO, GSO, 10Gbps Performance, rmem_max"}]],"seo":{"proficiencyLevel":"Expert","keywords":["Kernel Tuning","UDP Optimization","CPU Affinity","NIC Offloading","10Gbps Networking"]}},"headers":[],"relativePath":"en/kernel_tuning.md","filePath":"en/kernel_tuning.md","lastUpdated":1764595667000}'),t={name:"en/kernel_tuning.md"};function r(l,e,o,h,p,c){return a(),i("div",null,[...e[0]||(e[0]=[n(`<h1 id="linux-kernel-level-performance-tuning" tabindex="-1">Linux Kernel-Level Performance Tuning <a class="header-anchor" href="#linux-kernel-level-performance-tuning" aria-label="Permalink to “Linux Kernel-Level Performance Tuning”">​</a></h1><p>While DDC is designed for high performance with its Rust implementation, <a href="/en/software_architecture#lock-free-buffering-bipbuffer">lock-free buffers</a>, and efficient data handling, achieving maximum throughput, especially at <strong>1Gbps or 10Gbps Line Rates</strong>, often requires tuning the underlying operating system kernel. DDC operates at the user-space application layer, but its performance is fundamentally limited by how efficiently the Linux kernel handles network packets and CPU resources.</p><p>This guide focuses on Linux kernel parameters and system configurations that can significantly impact DDC&#39;s <a href="/en/protocol">UDP-based transport</a> performance.</p><h2 id="udp-socket-buffer-sizes" tabindex="-1">UDP Socket Buffer Sizes <a class="header-anchor" href="#udp-socket-buffer-sizes" aria-label="Permalink to “UDP Socket Buffer Sizes”">​</a></h2><p>The most common bottleneck for high-speed UDP applications is the kernel&#39;s default buffer sizes for UDP sockets. If the kernel&#39;s receive buffer fills up before DDC (or any user-space application) can read the packets, the kernel will silently drop incoming packets. Similarly, a small send buffer can limit how much data DDC can queue for transmission.</p><h3 id="key-sysctl-parameters" tabindex="-1">Key <code>sysctl</code> Parameters: <a class="header-anchor" href="#key-sysctl-parameters" aria-label="Permalink to “Key sysctl Parameters:”">​</a></h3><ul><li><strong><code>net.core.rmem_max</code></strong>: Maximum size of the receive buffer in bytes for all types of sockets.</li><li><strong><code>net.core.wmem_max</code></strong>: Maximum size of the send buffer in bytes for all types of sockets.</li><li><strong><code>net.core.rmem_default</code></strong>: Default size of the receive buffer for all types of sockets.</li><li><strong><code>net.core.wmem_default</code></strong>: Default size of the send buffer for all types of sockets.</li></ul><p><em>Example Recommendation (for 10Gbps+ environments):</em></p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Temporarily set (resets on reboot)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sysctl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -w</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> net.core.rmem_max=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">67108864</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">   # 64 MB</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sysctl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -w</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> net.core.wmem_max=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">67108864</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">   # 64 MB</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sysctl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -w</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> net.core.rmem_default=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">67108864</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sysctl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -w</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> net.core.wmem_default=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">67108864</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># For permanent change, add to /etc/sysctl.conf</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># net.core.rmem_max = 67108864</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># net.core.wmem_max = 67108864</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># net.core.rmem_default = 67108864</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># net.core.wmem_default = 67108864</span></span></code></pre></div><h3 id="monitoring-udp-buffer-overruns" tabindex="-1">Monitoring UDP Buffer Overruns: <a class="header-anchor" href="#monitoring-udp-buffer-overruns" aria-label="Permalink to “Monitoring UDP Buffer Overruns:”">​</a></h3><p>Use <code>netstat -su</code> or <code>ss -su</code> to monitor receive and send buffer errors. Look for the &quot;receive buffer errors&quot; or &quot;packets dropped&quot; counts. This is a key step in <a href="/en/operations_guide#troubleshooting">troubleshooting packet loss</a>.</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">netstat</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -su</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># ...</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Udp:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#     ...</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#     65535 packets dropped</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#     ...</span></span></code></pre></div><h2 id="cpu-affinity-and-interrupts" tabindex="-1">CPU Affinity and Interrupts <a class="header-anchor" href="#cpu-affinity-and-interrupts" aria-label="Permalink to “CPU Affinity and Interrupts”">​</a></h2><p>Network packet processing, especially on multi-core systems, can benefit significantly from CPU pinning and interrupt balancing.</p><h3 id="cpu-affinity-for-ddc-processes" tabindex="-1">CPU Affinity for DDC Processes: <a class="header-anchor" href="#cpu-affinity-for-ddc-processes" aria-label="Permalink to “CPU Affinity for DDC Processes:”">​</a></h3><p>Dedicate specific CPU cores to the <a href="/en/software_architecture">DDC Ingress/Egress</a> processes. This reduces context switching and ensures that DDC has exclusive access to CPU resources.</p><p><em>Example (pin process to CPU 1 and 2):</em></p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> taskset</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -c</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 1,2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">ddc_process_i</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span></code></pre></div><h3 id="interrupt-request-irq-affinity" tabindex="-1">Interrupt Request (IRQ) Affinity: <a class="header-anchor" href="#interrupt-request-irq-affinity" aria-label="Permalink to “Interrupt Request (IRQ) Affinity:”">​</a></h3><p>Ensure that network card interrupts are processed by dedicated CPU cores, ideally different from those running DDC processes. This prevents the DDC application from contending with NIC interrupts for CPU time. Tools like <code>irqbalance</code> can automate this, but for critical high-performance setups, manual pinning via <code>/proc/irq/&lt;IRQ_NUMBER&gt;/smp_affinity</code> might be necessary.</p><h2 id="network-interface-card-nic-offloading" tabindex="-1">Network Interface Card (NIC) Offloading <a class="header-anchor" href="#network-interface-card-nic-offloading" aria-label="Permalink to “Network Interface Card (NIC) Offloading”">​</a></h2><p>Modern NICs can offload various tasks from the CPU, significantly improving network performance. However, if DDC were to perform <a href="/en/security_model#content-filtering-and-sanitization">Deep Packet Inspection (DPI)</a> at the raw packet level, some offloading features might interfere. For DDC&#39;s current model (where protocol handlers process already received application data), offloading is generally beneficial.</p><h3 id="key-ethtool-parameters" tabindex="-1">Key <code>ethtool</code> Parameters: <a class="header-anchor" href="#key-ethtool-parameters" aria-label="Permalink to “Key ethtool Parameters:”">​</a></h3><ul><li><strong>Generic Receive Offload (GRO)</strong>: Consolidates multiple incoming packets into a larger super-packet before passing them to the kernel. This reduces the number of packets the kernel has to process.</li><li><strong>Generic Segmentation Offload (GSO)</strong>: Does the opposite for outgoing packets, allowing DDC to send a large data chunk that the NIC then segments into multiple smaller packets.</li><li><strong>Checksum Offload (Tx/Rx)</strong>: NIC calculates/verifies checksums instead of the CPU.</li></ul><p><em>Example (check status and enable):</em></p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check current status for an interface (e.g., eth0)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ethtool</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -k</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> eth0</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Enable GRO/GSO/Checksums (often enabled by default)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ethtool</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -K</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> eth0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gro</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> on</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> gso</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> on</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> tx-checksum-ip-generic</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> on</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rx-checksum-ip-generic</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> on</span></span></code></pre></div><h2 id="interrupt-coalescing" tabindex="-1">Interrupt Coalescing <a class="header-anchor" href="#interrupt-coalescing" aria-label="Permalink to “Interrupt Coalescing”">​</a></h2><p>Interrupt coalescing groups multiple hardware interrupts from the NIC into a single CPU interrupt. This reduces CPU overhead but can slightly increase latency.</p><h3 id="key-ethtool-parameters-1" tabindex="-1">Key <code>ethtool</code> Parameters: <a class="header-anchor" href="#key-ethtool-parameters-1" aria-label="Permalink to “Key ethtool Parameters:”">​</a></h3><ul><li><strong><code>rx-frames</code></strong>: Number of received frames before an interrupt is generated.</li><li><strong><code>rx-usecs</code></strong>: Microseconds to wait before generating an interrupt.</li></ul><p>Adjusting these parameters is a trade-off: higher values (more coalescing) lead to lower CPU utilization but higher latency; lower values (less coalescing) lead to higher CPU utilization but lower latency. For DDC, especially when <a href="/en/flow_control#step-4-the-0-trap"><code>send_delay_ms=0</code></a>, a balance is key.</p><p><em>Example (adjust coalescing):</em></p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Check current settings</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ethtool</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -c</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> eth0</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Set a custom value (e.g., more aggressive coalescing)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ethtool</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -C</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> eth0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rx-frames</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 256</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rx-usecs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 250</span></span></code></pre></div><h2 id="jumbo-frames" tabindex="-1">Jumbo Frames <a class="header-anchor" href="#jumbo-frames" aria-label="Permalink to “Jumbo Frames”">​</a></h2><p>If your entire network path (NICs, switches, routers) supports it, enabling Jumbo Frames (larger MTU, e.g., 9000 bytes instead of 1500) can reduce packet overhead and CPU cycles by allowing more data per packet.</p><p><em>Example (set MTU to 9000):</em></p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> link</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> set</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> dev</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> eth0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> mtu</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 9000</span></span></code></pre></div><blockquote><p><strong>Warning</strong>: Ensure all devices in the path support the larger MTU, otherwise, packets will be fragmented or dropped, leading to worse performance.</p></blockquote><h2 id="monitoring-tools" tabindex="-1">Monitoring Tools <a class="header-anchor" href="#monitoring-tools" aria-label="Permalink to “Monitoring Tools”">​</a></h2><p>To effectively tune, continuous monitoring is crucial:</p><ul><li><strong><code>netstat -su</code> / <code>ss -su</code></strong>: UDP packet statistics, including dropped packets.</li><li><strong><code>sar -n UDP 1</code></strong>: Real-time UDP traffic and errors.</li><li><strong><code>mpstat -P ALL 1</code></strong>: Per-CPU utilization.</li><li><strong><code>top</code> / <code>htop</code></strong>: Overall system resource usage.</li><li><strong><code>dmesg</code></strong>: Kernel logs for network-related errors.</li><li><strong><code>/proc/interrupts</code></strong>: Interrupt counts per CPU.</li></ul><h2 id="conclusion" tabindex="-1">Conclusion <a class="header-anchor" href="#conclusion" aria-label="Permalink to “Conclusion”">​</a></h2><p>Optimizing DDC performance, particularly for high-throughput scenarios, extends beyond <a href="/en/configuration_reference">application-level configuration</a>. By strategically tuning kernel parameters related to UDP buffering, CPU scheduling, NIC offloading, and interrupt handling, administrators can unlock the full potential of their hardware and achieve reliable, high-speed data transfer across the data diode. Always test changes thoroughly in a controlled environment.</p>`,43)])])}const g=s(t,[["render",r]]);export{k as __pageData,g as default};
