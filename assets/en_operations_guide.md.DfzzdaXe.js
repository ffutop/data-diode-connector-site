import{_ as t,c as o,o as a,a6 as i}from"./chunks/framework.Cpr3xyXy.js";const u=JSON.parse('{"title":"Operations Guide","description":"","frontmatter":{},"headers":[],"relativePath":"en/operations_guide.md","filePath":"en/operations_guide.md","lastUpdated":1764468807000}'),n={name:"en/operations_guide.md"};function s(r,e,l,d,c,g){return a(),o("div",null,[...e[0]||(e[0]=[i('<h1 id="operations-guide" tabindex="-1">Operations Guide <a class="header-anchor" href="#operations-guide" aria-label="Permalink to “Operations Guide”">​</a></h1><p>This document provides guidelines for deploying, monitoring, and troubleshooting the Data Diode Connector (DDC) in a production environment.</p><h2 id="deployment-checklist" tabindex="-1">Deployment Checklist <a class="header-anchor" href="#deployment-checklist" aria-label="Permalink to “Deployment Checklist”">​</a></h2><p>Before going live, ensure the following:</p><ol><li><strong>Network Reachability</strong>: <ul><li><strong>Ingress</strong>: Can reach the Source (e.g., Kafka) and the Diode Hardware/Network Interface.</li><li><strong>Egress</strong>: Can reach the Diode Hardware/Network Interface and the Destination.</li></ul></li><li><strong>MTU Configuration</strong>: <ul><li><strong>Ingress</strong>: Ensure the network interface MTU is large enough for the UDP packets, or let IP fragmentation handle it (though DDC handles application-layer fragmentation, avoiding IP fragmentation is better for performance).</li></ul></li><li><strong>Rate Limiting (<code>sendDelayMs</code>)</strong>: <ul><li>This is the most critical setting.</li><li><strong>Too Low</strong>: You will overflow the diode receiver buffer, causing packet loss.</li><li><strong>Too High</strong>: You will introduce unnecessary latency and reduce throughput.</li><li><strong>Tuning</strong>: Start high (e.g., <code>10ms</code>) and decrease until you see packet loss in the Egress logs, then back off slightly.</li></ul></li><li><strong>Buffer Sizing (<code>bipBufferElementCount</code>)</strong>: <ul><li>Increase this if you expect bursty traffic from the source.</li></ul></li></ol><h2 id="monitoring" tabindex="-1">Monitoring <a class="header-anchor" href="#monitoring" aria-label="Permalink to “Monitoring”">​</a></h2><p>DDC is designed to be &quot;observable by default&quot;.</p><h3 id="syslog" tabindex="-1">Syslog <a class="header-anchor" href="#syslog" aria-label="Permalink to “Syslog”">​</a></h3><p>DDC sends structured logs to the configured <code>toHostSyslog</code>, <code>toPortSyslog</code>.</p><ul><li><strong>Info</strong>: Startup events, configuration summaries.</li><li><strong>Warn</strong>: Recoverable issues (e.g., Kafka connection lost and retrying).</li><li><strong>Error</strong>: Critical failures (e.g., invalid config, thread panic).</li></ul><h3 id="metrics-statsd" tabindex="-1">Metrics (StatsD) <a class="header-anchor" href="#metrics-statsd" aria-label="Permalink to “Metrics (StatsD)”">​</a></h3><p>DDC emits metrics to any StatsD-compatible collector (Prometheus StatsD Exporter, Telegraf, Datadog).</p><p><strong>Key Metrics:</strong></p><table tabindex="0"><thead><tr><th style="text-align:left;">Metric Name</th><th style="text-align:left;">Type</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;"><code>ddc.ingress.packets_sent</code></td><td style="text-align:left;">Counter</td><td style="text-align:left;">Total UDP packets sent to diode.</td></tr><tr><td style="text-align:left;"><code>ddc.ingress.bytes_sent</code></td><td style="text-align:left;">Counter</td><td style="text-align:left;">Total bytes sent.</td></tr><tr><td style="text-align:left;"><code>ddc.egress.packets_received</code></td><td style="text-align:left;">Counter</td><td style="text-align:left;">Total UDP packets received.</td></tr><tr><td style="text-align:left;"><code>ddc.egress.packet_loss</code></td><td style="text-align:left;">Counter</td><td style="text-align:left;"><strong>Critical</strong>. Increments when a sequence gap is detected. Should be 0 in a healthy system.</td></tr><tr><td style="text-align:left;"><code>ddc.dropped_packets</code></td><td style="text-align:left;">Counter</td><td style="text-align:left;">Packets rejected by Filters.</td></tr><tr><td style="text-align:left;"><code>ddc.buffer_full</code></td><td style="text-align:left;">Counter</td><td style="text-align:left;">Increments if the Ring Buffer is full (indicates bottlenecks).</td></tr></tbody></table><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to “Troubleshooting”">​</a></h2><h3 id="scenario-1-i-see-packet-loss-increasing-on-the-egress-side" tabindex="-1">Scenario 1: &quot;I see <code>packet_loss</code> increasing on the Egress side.&quot; <a class="header-anchor" href="#scenario-1-i-see-packet-loss-increasing-on-the-egress-side" aria-label="Permalink to “Scenario 1: &quot;I see packet_loss increasing on the Egress side.&quot;”">​</a></h3><ul><li><strong>Cause</strong>: The Ingress is sending data faster than the physical diode or the Egress server can process it.</li><li><strong>Fix</strong>: Increase <code>transportUdpSend.sendDelayMs</code> in the Ingress configuration. Even a small increase (e.g., from 0ms to 1ms) can drastically stabilize the link.</li></ul><h3 id="scenario-2-data-is-not-arriving-at-the-destination-but-no-errors-in-logs" tabindex="-1">Scenario 2: &quot;Data is not arriving at the destination, but no errors in logs.&quot; <a class="header-anchor" href="#scenario-2-data-is-not-arriving-at-the-destination-but-no-errors-in-logs" aria-label="Permalink to “Scenario 2: &quot;Data is not arriving at the destination, but no errors in logs.&quot;”">​</a></h3><ul><li><strong>Cause</strong>: Filters might be silently dropping data (if configured to do so), or the Egress is receiving data but failing to produce to the sink.</li><li><strong>Fix</strong>: <ol><li>Check <code>ddc.dropped_packets</code> metric.</li><li>Set <code>logLevel = &quot;Debug&quot;</code> on Egress to see if it&#39;s receiving UDP packets.</li><li>Verify the Egress <code>protocolHandler</code> configuration (correct Kafka topic/host).</li></ol></li></ul><h3 id="scenario-3-high-latency" tabindex="-1">Scenario 3: &quot;High Latency.&quot; <a class="header-anchor" href="#scenario-3-high-latency" aria-label="Permalink to “Scenario 3: &quot;High Latency.&quot;”">​</a></h3><ul><li><strong>Cause</strong>: Large <code>sendDelayMs</code> or extremely large buffers.</li><li><strong>Fix</strong>: Reduce <code>sendDelayMs</code> as much as possible without causing packet loss. Reduce <code>bipBufferElementCount</code> to process smaller batches more frequently.</li></ul><h3 id="scenario-4-kafka-consumer-group-rebalancing-constantly" tabindex="-1">Scenario 4: &quot;Kafka Consumer Group rebalancing constantly.&quot; <a class="header-anchor" href="#scenario-4-kafka-consumer-group-rebalancing-constantly" aria-label="Permalink to “Scenario 4: &quot;Kafka Consumer Group rebalancing constantly.&quot;”">​</a></h3><ul><li><strong>Cause</strong>: Processing is taking too long, causing Kafka to time out the consumer.</li><li><strong>Fix</strong>: Decrease <code>maxBytesPerPartition</code> in the Protocol Handler config to fetch smaller batches.</li></ul>',23)])])}const f=t(n,[["render",s]]);export{u as __pageData,f as default};
